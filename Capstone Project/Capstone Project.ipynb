{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "To Describe the average climate in Different states or on Different year\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_data='/home/workspace/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Plan is to explore the file GlobalLandTemperaturesByCity and the airport-codes_csv.csv and the us-cities-demographics.csv to get the relevent info regarding the cities and states in United States\n",
    "\n",
    "#### Describe and Gather Data \n",
    "1. GlobalLandTemperaturesByCity - This file inclueds temperature info by Cities in different point in time\n",
    "2. airport-codes_csv.csv - This file contains the list of airport codes based on the Cities and the kinds of airports\n",
    "3. us-cities-demographics.csv - This file has demographic informartion for US-Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog=spark.read.option(\"delimiter\", \";\").csv(\"us-cities-demographics.csv\",header=True)\n",
    "airport=spark.read.csv(\"airport-codes_csv.csv\",header=True)\n",
    "temperatureData=spark.read.csv(\"../../data2/GlobalLandTemperaturesByCity.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "The data has null values for specific Cities\n",
    "\n",
    "#### Cleaning Steps\n",
    "Dropped the Null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatureData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#filter the world Temperature Data for only the U.S. and drop duplicates and convert celcius temperature to fahrenheit\n",
    "usTemperatures=temperatureData.filter(temperatureData[\"Country\"]==\"United States\")\\\n",
    ".withColumn(\"year\",F.year(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"month\",F.month(temperatureData[\"dt\"]))\\\n",
    ".withColumn('start_time',F.to_date(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"avg_temp_fahrenheit\",temperatureData[\"AverageTemperature\"]*9/5+32)\\\n",
    "\n",
    "new_Temperatures=usTemperatures.select(\"year\",\"month\",F.round(F.col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                    F.round(F.col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\"City\",\"Country\",\"start_time\") \\\n",
    "                    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#new_Temperatures.groupBy([F.year(new_Temperatures[\"start_time\"])]).count().orderBy(F.desc('count')).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Springfield', count=9364),\n",
       " Row(City='Columbus', count=6358),\n",
       " Row(City='Aurora', count=6079),\n",
       " Row(City='Arlington', count=5563),\n",
       " Row(City='Peoria', count=5384)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_Temperatures.groupBy(['City']).count().orderBy(F.desc('count')).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=1756, month=5, avg_temp_celcius=None, avg_temp_fahrenheit=None, City='Newark', Country='United States', dt='1756-05-01'),\n",
       " Row(year=1767, month=7, avg_temp_celcius=22.1, avg_temp_fahrenheit=71.8, City='Newark', Country='United States', dt='1767-07-01'),\n",
       " Row(year=1771, month=2, avg_temp_celcius=-4.0, avg_temp_fahrenheit=24.8, City='Newark', Country='United States', dt='1771-02-01'),\n",
       " Row(year=1772, month=1, avg_temp_celcius=-3.7, avg_temp_fahrenheit=25.3, City='Newark', Country='United States', dt='1772-01-01'),\n",
       " Row(year=1774, month=6, avg_temp_celcius=20.0, avg_temp_fahrenheit=68.0, City='Newark', Country='United States', dt='1774-06-01')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_Temperatures.filter(new_Temperatures.City=='Newark').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_Temperatures = new_Temperatures \\\n",
    ".join(demog.select('City','State').distinct(),on='City',how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Climate info by City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_Temperatures = new_Temperatures.filter(new_Temperatures.avg_temp_celcius.isNotNull())\\\n",
    "                        .filter(new_Temperatures.avg_temp_fahrenheit.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Abilene', year=1836, month=9, avg_temp_celcius=21.9, avg_temp_fahrenheit=71.4, Country='United States', start_time=datetime.date(1836, 9, 1), State='Texas'),\n",
       " Row(City='Abilene', year=1837, month=2, avg_temp_celcius=7.7, avg_temp_fahrenheit=45.9, Country='United States', start_time=datetime.date(1837, 2, 1), State='Texas'),\n",
       " Row(City='Abilene', year=1849, month=7, avg_temp_celcius=27.5, avg_temp_fahrenheit=81.5, Country='United States', start_time=datetime.date(1849, 7, 1), State='Texas'),\n",
       " Row(City='Abilene', year=1850, month=8, avg_temp_celcius=28.8, avg_temp_fahrenheit=83.9, Country='United States', start_time=datetime.date(1850, 8, 1), State='Texas'),\n",
       " Row(City='Abilene', year=1866, month=11, avg_temp_celcius=11.1, avg_temp_fahrenheit=51.9, Country='United States', start_time=datetime.date(1866, 11, 1), State='Texas')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Temperatures.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "1. dim_time\n",
    "        - Year\n",
    "        - Month\n",
    "        - Date\n",
    "2. dim_city\n",
    "        - City\n",
    "        - State\n",
    "        - Country\n",
    "3. fact_temp\n",
    "        - City\n",
    "        - Date\n",
    "        - avg_temp_fahrenheit\n",
    "        - avg_temp_celcius\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Joining the new_Temperatures dataframe with demographics dataframe on City\n",
    "\n",
    "1. The fact table is loaded with partition by City, start_time on date to local storage.\n",
    "2. City Table is loaded with partitons by Country and state\n",
    "3. Time table is loaded with partitions by year\n",
    "4. Extract transformed state demographical information from city demographics table as demographics dimension table.\n",
    "5. Converted the dt from to pyspark Date type format\n",
    "6. Extracted the year and month from the Temperatures file\n",
    "7. Created time table based on year, month, start time \n",
    "8. Created City table based on the City and Countries mentioned in the Temperatures file and demographics file for United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_Temperatures = new_Temperatures \\\n",
    ".join(demog.select('City','State').distinct(),on='City',how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Time table write to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table = new_Temperatures.select(['year','month','start_time']).distinct()\n",
    "time_table.write.parquet(output_data+\"time/\",  mode=\"overwrite\",partitionBy=[\"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## City Table write to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_meta = new_Temperatures.select(['Country','State','City']).distinct()\n",
    "city_meta.write.parquet(output_data+\"city_meta/\",  mode=\"overwrite\",partitionBy=[\"Country\",\"State\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Fact Temperature Table write to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_temp = new_Temperatures.select(['avg_temp_celcius','avg_temp_fahrenheit','City','start_time'])\n",
    "fact_temp.write.parquet(output_data+\"fact_temp/\",  mode=\"overwrite\",partitionBy=[\"City\",\"start_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "Assuming if a new dataset for a new year comes in \n",
    "Steps \n",
    "1. Derive the time_table columns\n",
    "2. Join the dataframe to demographics dataset in City\n",
    "3. Overwrite the existing table for new set of data\n",
    "4. Using the Star Schema \n",
    "    1. The temperature information is based on time and City that is added in the fact table\n",
    "    2. The City is unique list of City for a given country from Temperatures information as we are only looking for united states it is highly  related to the Cities mentioned in the demographics file to get the State name\n",
    "    3. Time table is derived from the dt in the Temperatures file which has the start time , month, year\n",
    "    4. As the users can have flexibility to look the temperature based on States as well as Year or month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperatureData2=spark.read.csv(\"../../data2/GlobalLandTemperaturesByCity.csv\",header=True)\n",
    "demog2=spark.read.option(\"delimiter\", \";\").csv(\"us-cities-demographics.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#filter the world Temperature Data for only the U.S. and only == 2013 and drop duplicates and convert celcius temp to f\n",
    "usTemperatures2=temperatureData2.filter(temperatureData2[\"Country\"]==\"United States\")\\\n",
    ".withColumn(\"year\",F.year(temperatureData2[\"dt\"]))\\\n",
    ".withColumn(\"month\",F.month(temperatureData2[\"dt\"]))\\\n",
    ".withColumn('start_time',F.to_date(temperatureData2[\"dt\"]))\\\n",
    ".withColumn(\"avg_temp_fahrenheit\",temperatureData2[\"AverageTemperature\"]*9/5+32)\\\n",
    "\n",
    "new_Temperatures2=usTemperatures2.select(\"year\",\"month\",F.round(F.col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                    F.round(F.col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\"City\",\"Country\",\"start_time\") \\\n",
    "                    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_Temperatures2 = new_Temperatures2 \\\n",
    ".join(demog2.select('City','State').distinct(),on='City',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_temp2 = new_Temperatures2.select(['avg_temp_celcius','avg_temp_fahrenheit','City','start_time'])\n",
    "fact_temp2.write.parquet(output_data+\"fact_temp/\",  mode=\"overwrite\",partitionBy=[\"City\",\"start_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_meta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_meta.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataDictionary\n",
      "\n",
      "Time table\n",
      "1. start_time - DateType notnull primary key - date on which the temperature was measured\n",
      "2. month - integer  notnull - month on which the temperature was measured\n",
      "3. year - integer  notnull - year on which the temperature was measured\n",
      "\n",
      "City Table\n",
      "1. City - string notnull primary key - avg temperature of the City\n",
      "2. State - string notnull - State in which the City is locate\n",
      "3. Country - string not null - Country in which the State is located\n",
      "\n",
      "fact_temp\n",
      "1. start_time - string notnull primary_key - date on which the temperature was measured\n",
      "2. City - string notnull - City in which the temperature was measured \n",
      "3. avg_temp_fahrenheit - integer notnull - avg temperature in fahrenheit\n",
      "4. avg_temp_celcius - integer notnull - avg temperature in celcius"
     ]
    }
   ],
   "source": [
    "!cat Data-Dictionary.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. Spark is primarily used for data exploration and ETL for faster processing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. The data can be updated as and when the temprature for new year is available to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. Currently this ETL process is only using 1 node to process the Temperature data. If the data grows I would mostly us a distributed architecture by scaling out (increasing the number of nodes) and also if that is not optimal then I would also increase the type of nodes (increasing the memory/cpu for each node in cluster) to reduce the processing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "4. I would create a spark job and run it when new data is available. I would check the data source daily4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "5. I would check the types of queries that 100+ people would like to run based on which I would use Redshift (for ad hoc querying) or Apache Cassandra(for daily reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Purpose of the Final Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "What will the data be used for? Who will use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. Data Model would be used by different users for Daily weather check based on the user City also Could be used for Temperature comparisons between different cities and comparisons of the cities can be at the State level or Country level also base or time (year, month, day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
